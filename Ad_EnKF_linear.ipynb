{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPolrOf8q/uStQMMEhf1Bx8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BotaoJin/Code-for-Thesis/blob/main/Ad_EnKF_linear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNIYkHvvMUmG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T_ = 100\n",
        "sigma = 0.1\n",
        "gamma = 0.1\n",
        "x_dim = 2\n",
        "y_dim = 2\n",
        "mean = torch.zeros(x_dim)\n",
        "cov = torch.eye(x_dim)\n",
        "eps = 0.01"
      ],
      "metadata": {
        "id": "Ny7-ljTJVjyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def KFilter(theta):\n",
        "    x0 = torch.tensor([1., 2.])# initialization of X0\n",
        "    X = torch.zeros(T_, x_dim)\n",
        "    Y = torch.zeros(T_, y_dim)\n",
        "    A_theta = torch.diag(torch.tensor(theta))\n",
        "\n",
        "    for t in range(T_):\n",
        "        zeta = MultivariateNormal(mean, (sigma**2)*cov)\n",
        "        eta = MultivariateNormal(mean, (gamma**2)*cov)\n",
        "        if t == 0:\n",
        "            X[t,:] = x0@A_theta + zeta.sample()\n",
        "        else:\n",
        "            x = X[t-1,:]\n",
        "            X[t,:] = x@A_theta + zeta.sample()\n",
        "            \n",
        "        Y[t,:] = X[t,:] + eta.sample()\n",
        "        \n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "ixONa9I4VpAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generating data\n",
        "num_data_set = 1\n",
        "X = torch.zeros(T_, num_data_set, x_dim)\n",
        "Y = torch.zeros(T_, num_data_set, y_dim)\n",
        "for i in range(num_data_set):\n",
        "  X_data, Y_data = KFilter([.9, .8])\n",
        "  X[:, i, :], Y[:, i, :] = X_data, Y_data\n",
        "\n",
        "X, Y = X.mean(dim = -2), Y.mean(dim = -2)"
      ],
      "metadata": {
        "id": "VfuMxMRSVsNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")\n",
        "\n",
        "def EnKF_log_likelihood(theta, Y, N_ensem, x0 = torch.tensor([1.,2.])):\n",
        "    # Compute log_likelihood of theta\n",
        "    # Y: the observation from time 1 to time T, 2*T matrix, with each column represents the state at time t\n",
        "    # theta: the variable in log_likelihood\n",
        "    # N_ensem: number of particles\n",
        "    # x0: initialization\n",
        "    \n",
        "    log_likelihood = torch.tensor(0., device = device)\n",
        "    T = Y.shape[-2]\n",
        "    X = x0.expand((N_ensem, x_dim))\n",
        "\n",
        "    \n",
        "    for j in range(T):\n",
        "        # Forcast Step\n",
        "        X = X * theta\n",
        "        #X = X @ A_theta\n",
        "        X = X + MultivariateNormal(mean.expand(N_ensem, x_dim), (sigma**2)*cov).sample() # model error for X: dim = (N_ensem, x_dim)\n",
        "        X_m = X.mean(dim = -2).unsqueeze(-2) # dim = (1, x_dim)\n",
        "        X_ct = X - X_m\n",
        "        \n",
        "        # Analysis Step: for $A_{\\theta}$ is a linear operator\n",
        "        y_obs_j = Y[j].unsqueeze(-2) # dim = (1, y_dim)\n",
        "        y_obs_perturb = MultivariateNormal(y_obs_j.expand(N_ensem, y_dim), (gamma**2)*cov).sample()\n",
        "        \n",
        "        C_uu = 1/(N_ensem - 1)*X_ct.transpose(-1, -2)@X_ct # dim = (1, x_dim)\n",
        "        # In this model, setting H = I\n",
        "        HX = X\n",
        "        HX_m = X_m\n",
        "        HC = C_uu\n",
        "        HCH_T = HC\n",
        "        HCH_TR_chol = torch.linalg.cholesky(HCH_T + (gamma**2)*cov)\n",
        "        d = MultivariateNormal(HX_m.squeeze(-2), scale_tril = HCH_TR_chol)\n",
        "        log_likelihood += d.log_prob(y_obs_j.squeeze(-2))\n",
        "        \n",
        "        # Update X\n",
        "        pre = (y_obs_perturb-HX)@torch.cholesky_inverse(HCH_TR_chol)\n",
        "        X = X + pre@HC\n",
        "    \n",
        "    return X, log_likelihood"
      ],
      "metadata": {
        "id": "CpTibY3NVtlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def true_log_likelihood(Y, theta, m0 = torch.tensor([1., 2.]), C0 = torch.tensor([[1., 0.],[0., 1.]])):\n",
        "  # calculate the true log likelihood for theta\n",
        "\n",
        "  sum_log_likelihood = 0\n",
        "  J = Y.shape[-2]\n",
        "  Id = torch.tensor([[1., 0.], [0., 1]])\n",
        "  m = m0\n",
        "  C = C0\n",
        "\n",
        "  for j in range(J):\n",
        "    # prediction part\n",
        "    m_hat = theta * m # calculate \\hat{m_{j+1}}\n",
        "    C_hat = theta*C*theta.t() + (gamma**2)*cov\n",
        "\n",
        "    # Analysis part\n",
        "    dj = Y[j]-m_hat\n",
        "    S = C_hat + (gamma**2)*cov\n",
        "    S_inv = torch.linalg.inv(S)\n",
        "    K = C_hat@S_inv\n",
        "    m = m_hat + dj@K.t()\n",
        "    C = (Id - K)@C_hat\n",
        "\n",
        "    norm_error = (Y[j]-m_hat)@S_inv@(Y[j]-m_hat).t()\n",
        "    sum_log_likelihood += 2 * torch.log(torch.tensor(2*math.pi))\n",
        "    sum_log_likelihood += norm_error\n",
        "    sum_log_likelihood += torch.log(torch.det(S))\n",
        "\n",
        "  sum_log_likelihood *= (-1/2)\n",
        "\n",
        "  return sum_log_likelihood"
      ],
      "metadata": {
        "id": "vAvml6HkV0PL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient ascent (N_ensem = 20)\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "\n",
        "alpha = .2\n",
        "eta = 2e-4 # learning rate\n",
        "theta = torch.tensor([0., 0.]) # initial value of theta\n",
        "k = 1\n",
        "diff = 1\n",
        "eps = 1e-10\n",
        "n_iterations = 200\n",
        "iter1_theta1 = []\n",
        "iter1_theta2 = []\n",
        "grad1_theta1 = []\n",
        "grad1_theta2 = []\n",
        "log_like1_theta = []\n",
        "true_log_like1 = []\n",
        "\n",
        "while k <= n_iterations and diff > eps:\n",
        "    theta_k = theta.clone().detach().requires_grad_(True)\n",
        "    X__, L = EnKF_log_likelihood(theta_k, Y, N_ensem = 20)\n",
        "    log_like1_theta.append(L)\n",
        "    L.backward()\n",
        "    grad_log_likelihood = theta_k.grad\n",
        "    grad1_theta1.append(grad_log_likelihood[0])\n",
        "    grad1_theta2.append(grad_log_likelihood[1])\n",
        "    k = k+1\n",
        "    #theta_new = theta + eta * grad_log_likelihood\n",
        "    theta_new = theta + eta*(k**(-alpha)) * grad_log_likelihood\n",
        "    diff = torch.linalg.norm(theta_new - theta)\n",
        "    true_log_like_ = true_log_likelihood(Y, theta)\n",
        "    theta = theta_new\n",
        "    true_log_like1.append(true_log_like_)\n",
        "    iter1_theta1.append(theta[0])\n",
        "    iter1_theta2.append(theta[1])"
      ],
      "metadata": {
        "id": "fYVR4ZfWV--H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient ascent (N_ensem = 200)\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "\n",
        "alpha = .2\n",
        "eta = 2e-4 # learning rate\n",
        "theta = torch.tensor([0., 0.]) # initial value of theta\n",
        "k = 1\n",
        "diff = 1\n",
        "eps = 1e-10\n",
        "n_iterations = 200\n",
        "iter2_theta1 = []\n",
        "iter2_theta2 = []\n",
        "grad2_theta1 = []\n",
        "grad2_theta2 = []\n",
        "log_like2_theta = []\n",
        "true_log_like2 = []\n",
        "\n",
        "while k <= n_iterations and diff > eps:\n",
        "    theta_k = theta.clone().detach().requires_grad_(True)\n",
        "    X__, L = EnKF_log_likelihood(theta_k, Y, N_ensem = 200)\n",
        "    log_like2_theta.append(L)\n",
        "    L.backward()\n",
        "    grad_log_likelihood = theta_k.grad\n",
        "    grad2_theta1.append(grad_log_likelihood[0])\n",
        "    grad2_theta2.append(grad_log_likelihood[1])\n",
        "    k = k+1\n",
        "    #theta_new = theta + eta * grad_log_likelihood\n",
        "    theta_new = theta + eta*(k**(-alpha)) * grad_log_likelihood\n",
        "    diff = torch.linalg.norm(theta_new - theta)\n",
        "    true_log_like_ = true_log_likelihood(Y, theta)\n",
        "    theta = theta_new\n",
        "    true_log_like2.append(true_log_like_)\n",
        "    iter2_theta1.append(theta[0])\n",
        "    iter2_theta2.append(theta[1])"
      ],
      "metadata": {
        "id": "pvKEND6QWCp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient ascent (N_ensem = 2000)\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "\n",
        "alpha = .2\n",
        "eta = 2e-4 # learning rate\n",
        "theta = torch.tensor([0., 0.]) # initial value of theta\n",
        "k = 1\n",
        "diff = 1\n",
        "eps = 1e-10\n",
        "n_iterations = 200\n",
        "iter3_theta1 = []\n",
        "iter3_theta2 = []\n",
        "grad3_theta1 = []\n",
        "grad3_theta2 = []\n",
        "log_like3_theta = []\n",
        "true_log_like3 = []\n",
        "\n",
        "while k <= n_iterations and diff > eps:\n",
        "    theta_k = theta.clone().detach().requires_grad_(True)\n",
        "    X__, L = EnKF_log_likelihood(theta_k, Y, N_ensem = 2000)\n",
        "    log_like3_theta.append(L)\n",
        "    L.backward()\n",
        "    grad_log_likelihood = theta_k.grad\n",
        "    grad3_theta1.append(grad_log_likelihood[0])\n",
        "    grad3_theta2.append(grad_log_likelihood[1])\n",
        "    k = k+1\n",
        "    #theta_new = theta + eta * grad_log_likelihood\n",
        "    theta_new = theta + eta*(k**(-alpha)) * grad_log_likelihood\n",
        "    diff = torch.linalg.norm(theta_new - theta)\n",
        "    true_log_like_ = true_log_likelihood(Y, theta)\n",
        "    theta = theta_new\n",
        "    true_log_like3.append(true_log_like_)\n",
        "    iter3_theta1.append(theta[0])\n",
        "    iter3_theta2.append(theta[1])"
      ],
      "metadata": {
        "id": "8uuLYIzwWGPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure(figsize=(18, 4.5))\n",
        "\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.plot(range(len(log_like1_theta)), torch.tensor(log_like1_theta).detach().numpy(), label = 'EnKF log-like, N_ensem = 20')\n",
        "plt.plot(range(len(true_log_like1)), torch.tensor(true_log_like1).detach().numpy(), label = 'True log-like, N_ensem = 20')\n",
        "plt.plot(range(len(log_like2_theta)), torch.tensor(log_like2_theta).detach().numpy(), label = 'EnKF log-like, N_ensem = 200')\n",
        "plt.plot(range(len(true_log_like2)), torch.tensor(true_log_like2).detach().numpy(), label = 'True log-like, N_ensem = 200')\n",
        "plt.plot(range(len(log_like3_theta)), torch.tensor(log_like3_theta).detach().numpy(), label = 'EnKF log-like, N_ensem = 2000')\n",
        "plt.plot(range(len(true_log_like3)), torch.tensor(true_log_like3).detach().numpy(), label = 'True log-like, N_ensem = 2000')\n",
        "plt.ylabel('log likelihood')\n",
        "plt.xlabel('iterations')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.plot(range(len(grad1_theta1)), torch.tensor(grad1_theta1).detach().numpy(), label = 'grad, N_ensem = 20')\n",
        "plt.plot(range(len(grad2_theta1)), torch.tensor(grad2_theta1).detach().numpy(), label = 'grad, N_ensem = 200')\n",
        "plt.plot(range(len(grad3_theta1)), torch.tensor(grad3_theta1).detach().numpy(), label = 'grad, N_ensem = 2000')\n",
        "plt.ylabel('grad of theta1')\n",
        "plt.xlabel('iterations')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.plot(range(len(grad1_theta2)), torch.tensor(grad1_theta2).detach().numpy(), label = 'grad, N_ensem = 20')\n",
        "plt.plot(range(len(grad2_theta2)), torch.tensor(grad2_theta2).detach().numpy(), label = 'grad, N_ensem = 200')\n",
        "plt.plot(range(len(grad3_theta2)), torch.tensor(grad3_theta2).detach().numpy(), label = 'grad, N_ensem = 2000')\n",
        "plt.ylabel('grad of theta2')\n",
        "plt.xlabel('iterations')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N0BJyOrRrgDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(12, 4.5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(range(len(iter1_theta1)), torch.tensor(iter1_theta1).clone().numpy(), label = 'values of theta1, N_ensem = 20')\n",
        "plt.plot(range(len(iter2_theta1)), torch.tensor(iter2_theta1).clone().numpy(), label = 'values of theta1, N_ensem = 200')\n",
        "plt.plot(range(len(iter3_theta1)), torch.tensor(iter3_theta1).clone().numpy(), label = 'values of theta1, N_ensem = 2000')\n",
        "plt.plot(range(len(iter1_theta1)), .9*np.ones((len(iter1_theta1),)), label = 'true value of theta1')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('values')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(range(len(iter1_theta2)), torch.tensor(iter1_theta2).clone().numpy(), label = 'values of theta2, N_ensem = 20')\n",
        "plt.plot(range(len(iter2_theta2)), torch.tensor(iter2_theta2).clone().numpy(), label = 'values of theta2, N_ensem = 200')\n",
        "plt.plot(range(len(iter3_theta2)), torch.tensor(iter3_theta2).clone().numpy(), label = 'values of theta2, N_ensem = 2000')\n",
        "plt.plot(range(len(iter1_theta2)), .8*np.ones((len(iter1_theta2),)), label = 'true value of theta2')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('values')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NO5tFPl10Rhj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}