{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqkQI32e/Hj+hw3LG5m81I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BotaoJin/Code-for-Thesis/blob/main/AD_EnKF_linear_TBP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlseXQjk3JKh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T_ = 100\n",
        "sigma = 0.1\n",
        "gamma = 0.1\n",
        "x_dim = 2\n",
        "y_dim = 2\n",
        "mean = torch.zeros(x_dim)\n",
        "cov = torch.eye(x_dim)\n",
        "eps = 0.01"
      ],
      "metadata": {
        "id": "VyuNCL4o3Tce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def KFilter(theta):\n",
        "    x0 = torch.tensor([1., 2.])# initialization of X0\n",
        "    X = torch.zeros(T_, x_dim)\n",
        "    Y = torch.zeros(T_, y_dim)\n",
        "    A_theta = torch.diag(torch.tensor(theta))\n",
        "\n",
        "    for t in range(T_):\n",
        "        zeta = MultivariateNormal(mean, (sigma**2)*cov)\n",
        "        eta = MultivariateNormal(mean, (gamma**2)*cov)\n",
        "        if t == 0:\n",
        "            X[t,:] = x0@A_theta + zeta.sample()\n",
        "        else:\n",
        "            x = X[t-1,:]\n",
        "            X[t,:] = x@A_theta + zeta.sample()\n",
        "            \n",
        "        Y[t,:] = X[t,:] + eta.sample()\n",
        "        \n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "NXpw6RVX3WWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generating data\n",
        "num_data_set = 2\n",
        "X = torch.zeros(T_, num_data_set, x_dim)\n",
        "Y = torch.zeros(T_, num_data_set, y_dim)\n",
        "for i in range(num_data_set):\n",
        "  X_data, Y_data = KFilter([.9, .8])\n",
        "  X[:, i, :], Y[:, i, :] = X_data, Y_data\n",
        "\n",
        "X, Y = X.mean(dim = -2), Y.mean(dim = -2)"
      ],
      "metadata": {
        "id": "wwoARTir3WfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")\n",
        "\n",
        "def EnKF_log_likelihood(theta, Y, N_ensem, x0 = torch.tensor([1.,2.])):\n",
        "    # Compute log_likelihood of theta\n",
        "    # Y: the observation from time 1 to time T, 2*T matrix, with each column represents the state at time t\n",
        "    # theta: the variable in log_likelihood\n",
        "    # N_ensem: number of particles\n",
        "    # x0: initialization\n",
        "    \n",
        "    log_likelihood = torch.tensor(0., device = device)\n",
        "    T = Y.shape[-2]\n",
        "    X = x0.expand((N_ensem, x_dim))\n",
        "\n",
        "    \n",
        "    for j in range(T):\n",
        "        # Forcast Step\n",
        "        X = X * theta\n",
        "        #X = X @ A_theta\n",
        "        X = X + MultivariateNormal(mean.expand(N_ensem, x_dim), (sigma**2)*cov).sample() # model error for X: dim = (N_ensem, x_dim)\n",
        "        X_m = X.mean(dim = -2).unsqueeze(-2) # dim = (1, x_dim)\n",
        "        X_ct = X - X_m\n",
        "        \n",
        "        # Analysis Step: for $A_{\\theta}$ is a linear operator\n",
        "        y_obs_j = Y[j].unsqueeze(-2) # dim = (1, y_dim)\n",
        "        y_obs_perturb = MultivariateNormal(y_obs_j.expand(N_ensem, y_dim), (gamma**2)*cov).sample()\n",
        "        \n",
        "        C_uu = 1/(N_ensem - 1)*X_ct.transpose(-1, -2)@X_ct # dim = (1, x_dim)\n",
        "        # In this model, setting H = I\n",
        "        HX = X\n",
        "        HX_m = X_m\n",
        "        HC = C_uu\n",
        "        HCH_T = HC\n",
        "        HCH_TR_chol = torch.linalg.cholesky(HCH_T + (gamma**2)*cov)\n",
        "        d = MultivariateNormal(HX_m.squeeze(-2), scale_tril = HCH_TR_chol)\n",
        "        log_likelihood += d.log_prob(y_obs_j.squeeze(-2))\n",
        "        \n",
        "        # Update X\n",
        "        pre = (y_obs_perturb-HX)@torch.cholesky_inverse(HCH_TR_chol)\n",
        "        X = X + pre@HC\n",
        "    \n",
        "    return X, log_likelihood"
      ],
      "metadata": {
        "id": "OqRPSno_3dK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def true_log_likelihood(Y, theta, m0 = torch.tensor([1., 2.]), C0 = torch.tensor([[1., 0.],[0., 1.]])):\n",
        "  # calculate the true log likelihood for theta\n",
        "\n",
        "  sum_log_likelihood = 0\n",
        "  J = Y.shape[-2]\n",
        "  Id = torch.tensor([[1., 0.], [0., 1]])\n",
        "  m = m0\n",
        "  C = C0\n",
        "\n",
        "  for j in range(J):\n",
        "    # prediction part\n",
        "    m_hat = theta * m # calculate \\hat{m_{j+1}}\n",
        "    C_hat = theta*C*theta.t() + (gamma**2)*cov\n",
        "\n",
        "    # Analysis part\n",
        "    dj = Y[j]-m_hat\n",
        "    S = C_hat + (gamma**2)*cov\n",
        "    S_inv = torch.linalg.inv(S)\n",
        "    K = C_hat@S_inv\n",
        "    m = m_hat + dj@K.t()\n",
        "    C = (Id - K)@C_hat\n",
        "\n",
        "    norm_error = (Y[j]-m_hat)@S_inv@(Y[j]-m_hat).t()\n",
        "    sum_log_likelihood += 2 * torch.log(torch.tensor(2*math.pi))\n",
        "    sum_log_likelihood += norm_error\n",
        "    sum_log_likelihood += torch.log(torch.det(S))\n",
        "\n",
        "  sum_log_likelihood *= (-1/2)\n",
        "\n",
        "  return m, C, sum_log_likelihood"
      ],
      "metadata": {
        "id": "i6mmKHsV2vMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eta = 5e-4 # learning rate\n",
        "theta = torch.tensor([0., 0.]) # initial value of theta\n",
        "diff = 1\n",
        "n_iterations = 100\n",
        "# In the model, given that T = 100\n",
        "Length = 20 # subsequence length is 20\n",
        "iter_theta1_TBP = []\n",
        "iter_theta2_TBP = []\n",
        "grad_theta1_TBP = []\n",
        "grad_theta2_TBP = []\n",
        "log_like_theta_TBP = []\n",
        "true_log_like_TBP = []\n",
        "\n",
        "for k in range(n_iterations):\n",
        "  x0 = m0 = torch.tensor([1., 2.])\n",
        "  C0 = torch.tensor([[1., 0.],[0., 1.]])\n",
        "  x = x0\n",
        "  m = m0\n",
        "  C = C0\n",
        "  for j in range(int(T_/Length)):\n",
        "    theta_k = theta.clone().detach().requires_grad_(True)\n",
        "    t0 = j*Length\n",
        "    t1 = np.minimum((j+1)*Length, T_)\n",
        "    y = Y[t0:t1]\n",
        "\n",
        "    x, L = EnKF_log_likelihood(theta_k, y, N_ensem = 3000, x0 = x) # calculate likelihood, and update x_t\n",
        "    log_like_theta_TBP.append(L)\n",
        "    L.backward(retain_graph=True)\n",
        "    grad_log_likelihood = theta_k.grad\n",
        "    grad_theta1_TBP.append(grad_log_likelihood[0])\n",
        "    grad_theta2_TBP.append(grad_log_likelihood[1])\n",
        "\n",
        "    m, C, true_L = true_log_likelihood(y, theta_k, m0 = m, C0 = C)\n",
        "    true_log_like_TBP.append(true_L)\n",
        "\n",
        "    theta = theta_k + eta * torch.tensor([1.5, 1.])* grad_log_likelihood\n",
        "    iter_theta1_TBP.append(theta[0])\n",
        "    iter_theta2_TBP.append(theta[1])"
      ],
      "metadata": {
        "id": "LfsRtMUd3qv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(18, 4.5))\n",
        "n1 = 5*np.arange(100)\n",
        "n2 = 5*np.arange(100)+1\n",
        "n3 = 5*np.arange(100)+2\n",
        "n4 = 5*np.arange(100)+3\n",
        "n5 = 5*np.arange(100)+4\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.plot(n1, torch.tensor(log_like_theta_TBP).detach().numpy()[n1], label = 'EnKF log-like(n1)')\n",
        "plt.plot(n2, torch.tensor(log_like_theta_TBP).detach().numpy()[n2], label = 'EnKF log-like(n2)')\n",
        "plt.plot(n3, torch.tensor(log_like_theta_TBP).detach().numpy()[n3], label = 'EnKF log-like(n3)')\n",
        "plt.plot(n4, torch.tensor(log_like_theta_TBP).detach().numpy()[n4], label = 'EnKF log-like(n4)')\n",
        "plt.plot(n5, torch.tensor(log_like_theta_TBP).detach().numpy()[n5], label = 'EnKF log-like(n5)')\n",
        "plt.plot(n1, torch.tensor(true_log_like_TBP).detach().numpy()[n1], label = 'EnKF log-like(n1)')\n",
        "plt.plot(n2, torch.tensor(true_log_like_TBP).detach().numpy()[n2], label = 'EnKF log-like(n2)')\n",
        "plt.plot(n3, torch.tensor(true_log_like_TBP).detach().numpy()[n3], label = 'EnKF log-like(n3)')\n",
        "plt.plot(n4, torch.tensor(true_log_like_TBP).detach().numpy()[n4], label = 'EnKF log-like(n4)')\n",
        "plt.plot(n5, torch.tensor(true_log_like_TBP).detach().numpy()[n5], label = 'EnKF log-like(n5)')\n",
        "plt.ylabel('log likelihood')\n",
        "plt.xlabel('iterations')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.plot(n1, torch.tensor(grad_theta1_TBP).detach().numpy()[n1], label = 'grad n1')\n",
        "plt.plot(n2, torch.tensor(grad_theta1_TBP).detach().numpy()[n2], label = 'grad n2')\n",
        "plt.plot(n3, torch.tensor(grad_theta1_TBP).detach().numpy()[n3], label = 'grad n3')\n",
        "plt.plot(n4, torch.tensor(grad_theta1_TBP).detach().numpy()[n4], label = 'grad n4')\n",
        "plt.plot(n5, torch.tensor(grad_theta1_TBP).detach().numpy()[n5], label = 'grad n5')\n",
        "plt.ylabel('grad of theta1')\n",
        "plt.xlabel('iterations')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.plot(n1, torch.tensor(grad_theta2_TBP).detach().numpy()[n1], label = 'grad n1')\n",
        "plt.plot(n2, torch.tensor(grad_theta2_TBP).detach().numpy()[n2], label = 'grad n2')\n",
        "plt.plot(n3, torch.tensor(grad_theta2_TBP).detach().numpy()[n3], label = 'grad n3')\n",
        "plt.plot(n4, torch.tensor(grad_theta2_TBP).detach().numpy()[n4], label = 'grad n4')\n",
        "plt.plot(n5, torch.tensor(grad_theta2_TBP).detach().numpy()[n5], label = 'grad n5')\n",
        "plt.ylabel('grad of theta2')\n",
        "plt.xlabel('iterations')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R8jurxsT4Ki9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(12, 4.5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(range(len(iter_theta1_TBP)), torch.tensor(iter_theta1_TBP).clone().numpy(), label = 'values of theta1')\n",
        "plt.plot(range(len(iter_theta1_TBP)), .9*np.ones((len(iter_theta1_TBP),)), label = 'true value of theta1')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('values')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(range(len(iter_theta2_TBP)), torch.tensor(iter_theta2_TBP).clone().numpy(), label = 'values of theta2')\n",
        "plt.plot(range(len(iter_theta2_TBP)), .8*np.ones((len(iter_theta2_TBP),)), label = 'true value of theta2')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('values')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aVkIerz44Sgq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}