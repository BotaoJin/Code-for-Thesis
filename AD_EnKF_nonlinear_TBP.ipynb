{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOtDOOmbYoGZ1OTtFXsAyBA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BotaoJin/Code-for-Thesis/blob/main/AD_EnKF_nonlinear_TBP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lY3689RFsmZm",
        "outputId": "144d7025-cd09-430f-8b11-eb976cada633"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchdiffeq\n",
            "  Downloading torchdiffeq-0.2.3-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq) (1.7.3)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy>=1.4.0->torchdiffeq) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->torchdiffeq) (4.1.1)\n",
            "Installing collected packages: torchdiffeq\n",
            "Successfully installed torchdiffeq-0.2.3\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "!pip install torchdiffeq\n",
        "from torchdiffeq import odeint\n",
        "from torchdiffeq import odeint_adjoint\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Lorenz63(nn.Module):\n",
        "  def __init__(self, sigma, beta, rho, x_dim = 3):\n",
        "      super().__init__()\n",
        "      self.sigma = nn.Parameter(sigma)\n",
        "      self.beta = nn.Parameter(beta)\n",
        "      self.rho = nn.Parameter(rho)\n",
        "      self.x_dim = 3\n",
        "\n",
        "  def forward(self, t, u):\n",
        "    sigma = self.sigma\n",
        "    beta = self.beta\n",
        "    rho = self.rho\n",
        "    out = torch.stack((sigma*(u[...,1]-u[...,0]), rho*u[..., 0]-u[..., 0]*u[..., 2]-u[..., 1], u[..., 0]*u[..., 1]-beta*u[..., 2]), dim = -1)\n",
        "    return out"
      ],
      "metadata": {
        "id": "gulO57_7s4If"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_dim = 3\n",
        "y_dim = 3\n",
        "H = torch.eye(3)\n",
        "cov = torch.eye(3)\n",
        "gamma = .1\n",
        "T_ = 100\n",
        "t1 = torch.tensor([0., .01])\n",
        "\n",
        "def generate_data(ode_func):\n",
        "  x0 = torch.tensor([1., 1., 1.])\n",
        "  X = torch.empty(T_+1, x_dim)\n",
        "  Y = torch.empty(T_, y_dim)\n",
        "  X[0] = x0\n",
        "\n",
        "  for t in range(T_):\n",
        "    X[t+1] = odeint(ode_func, X[t], t1)[-1]\n",
        "    X[t+1] = MultivariateNormal(X[t+1], gamma**2*cov).sample()\n",
        "    Y[t] = MultivariateNormal(X[t+1], gamma**2*cov).sample()\n",
        "\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "h-gscyrws4N0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate data\n",
        "true_sigma = torch.tensor(10.0)\n",
        "true_beta = torch.tensor(8.0/3.0)\n",
        "true_rho = torch.tensor(28.0)\n",
        "\n",
        "true_ode_func = Lorenz63(sigma = true_sigma, beta = true_beta, rho = true_rho)\n",
        "X, Y = generate_data(true_ode_func)"
      ],
      "metadata": {
        "id": "em9yGjaEs4QC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EnKF(ode_func, Y, N_ensem, t_span, x0 = torch.tensor([1., 1., 1.])):\n",
        "  T = Y.shape[-2]\n",
        "  X = x0.expand((N_ensem, x_dim))\n",
        "  res = torch.empty(T+1, N_ensem, x_dim)\n",
        "  res[0] = X\n",
        "  mean = torch.zeros(x_dim)\n",
        "  log_likelihood = torch.tensor(0.)\n",
        "\n",
        "  for t in range(T):\n",
        "    # Forcast Step\n",
        "    X = odeint_adjoint(ode_func, X, t_span, method = 'rk4', adjoint_method = 'rk4')[-1]\n",
        "    X = X + MultivariateNormal(mean.expand((N_ensem, x_dim)), (gamma**2)*cov).sample() # model error for X: dim = (N_ensem, x_dim)\n",
        "    X_m = X.mean(dim = -2).unsqueeze(-2) # dim = (1, x_dim)\n",
        "    X_ct = X - X_m\n",
        "\n",
        "    # Analysis Step\n",
        "    y_obs_j = Y[t].unsqueeze(-2) # dim = (1, y_dim)\n",
        "    y_obs_perturb = MultivariateNormal(y_obs_j.expand(N_ensem, y_dim), (gamma**2)*cov).sample()\n",
        "\n",
        "    C_uu = 1/(N_ensem - 1)*X_ct.transpose(-1, -2)@X_ct # dim = (1, x_dim)\n",
        "    # In this model, setting H = I\n",
        "    HX = X\n",
        "    HX_m = X_m\n",
        "    HC = C_uu\n",
        "    HCH_T = HC\n",
        "    HCH_TR_chol = torch.linalg.cholesky(HCH_T + (gamma**2)*cov)\n",
        "    d = MultivariateNormal(HX_m.squeeze(-2), scale_tril = HCH_TR_chol)\n",
        "    log_likelihood += d.log_prob(y_obs_j.squeeze(-2))\n",
        "\n",
        "    # Update and store X_j^{1:N}\n",
        "    pre = (y_obs_perturb-HX)@torch.cholesky_inverse(HCH_TR_chol)\n",
        "    X = X + pre@HC\n",
        "    res[t+1] = X\n",
        "\n",
        "  return X, res, log_likelihood"
      ],
      "metadata": {
        "id": "N-1LfbQYs4SO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient ascent (TBP, sigma unknown, N_ensem = 3000)\n",
        "sigma, beta, rho = torch.tensor(.01), true_beta, true_rho\n",
        "iter_sigma_GD = []\n",
        "iter_grad_sigma = []\n",
        "eta_sigma = 1.5e-3\n",
        "L = 20\n",
        "\n",
        "for i in range(100):\n",
        "  x_bar = torch.tensor([1., 1., 1.])\n",
        "  for j in range(int(T_/L)):\n",
        "    t_start = j*L\n",
        "    t_end = np.minimum((j+1)*L, T_)\n",
        "    y = Y[t_start:t_end]\n",
        "\n",
        "    sigma1 = sigma.clone().detach().requires_grad_(True)\n",
        "\n",
        "    ode_func = Lorenz63(sigma1, beta, rho)\n",
        "    x_bar, res, loglike = EnKF(ode_func, y, N_ensem = 3000, t_span = t1, x0 = x_bar)\n",
        "    loglike.backward(retain_graph = True)\n",
        "\n",
        "    grad_sigma = ode_func.sigma.grad\n",
        "    sigma = sigma + eta_sigma*grad_sigma\n",
        "\n",
        "    iter_sigma_GD.append(sigma)\n",
        "    iter_grad_sigma.append(grad_sigma)"
      ],
      "metadata": {
        "id": "g7upEoY1s4UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient ascent (TBP, beta unknown, N_ensem = 3000)\n",
        "sigma, beta, rho = true_sigma, torch.tensor(0.), true_rho\n",
        "iter_beta_GD = []\n",
        "iter_grad_beta = []\n",
        "eta_beta = 2e-4\n",
        "L = 20\n",
        "\n",
        "for i in range(100):\n",
        "  x_bar = torch.tensor([1., 1., 1.])\n",
        "  for j in range(int(T_/L)):\n",
        "    t_start = j*L\n",
        "    t_end = np.minimum((j+1)*L, T_)\n",
        "    y = Y[t_start:t_end]\n",
        "\n",
        "    beta1 = beta.clone().detach().requires_grad_(True)\n",
        "\n",
        "    ode_func = Lorenz63(sigma, beta1, rho)\n",
        "    x_bar, res, loglike = EnKF(ode_func, y, N_ensem = 3000, t_span = t1, x0 = x_bar)\n",
        "    loglike.backward(retain_graph = True)\n",
        "    \n",
        "    grad_beta = ode_func.beta.grad\n",
        "    beta = beta + eta_beta*grad_beta\n",
        "\n",
        "    iter_beta_GD.append(beta)\n",
        "    iter_grad_beta.append(grad_beta)"
      ],
      "metadata": {
        "id": "XmbHQQeas4WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Ascent (TBP, rho unknown, N_ensem = 3000)\n",
        "sigma, beta, rho = true_sigma, true_beta, torch.tensor(0.)\n",
        "iter_rho_GD = []\n",
        "iter_grad_rho = []\n",
        "eta_rho = 1e-3\n",
        "L = 20\n",
        "\n",
        "for i in range(100):\n",
        "  x_bar = torch.tensor([1., 1., 1.])\n",
        "  for j in range(int(T_/L)):\n",
        "    t_start = j*L\n",
        "    t_end = np.minimum((j+1)*L, T_)\n",
        "    y = Y[t_start:t_end]\n",
        "\n",
        "    rho1 = rho.clone().detach().requires_grad_(True)\n",
        "\n",
        "    ode_func = Lorenz63(sigma, beta, rho1)\n",
        "    x_bar, res, loglike = EnKF(ode_func, y, N_ensem = 3000, t_span = t1, x0 = x_bar)\n",
        "    loglike.backward(retain_graph = True)\n",
        "\n",
        "    grad_rho = ode_func.rho.grad\n",
        "    rho = rho + eta_rho*grad_rho\n",
        "\n",
        "    iter_rho_GD.append(rho)\n",
        "    iter_grad_rho.append(grad_rho)"
      ],
      "metadata": {
        "id": "2i9pbk2_BHZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(15, 4.5))\n",
        "n1 = 5*np.arange(100)\n",
        "n2 = 5*np.arange(100)+1\n",
        "n3 = 5*np.arange(100)+2\n",
        "n4 = 5*np.arange(100)+3\n",
        "n5 = 5*np.arange(100)+4\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.plot(n1, torch.tensor(iter_grad_sigma).detach().numpy()[n1], label = 'grad n1')\n",
        "plt.plot(n2, torch.tensor(iter_grad_sigma).detach().numpy()[n2], label = 'grad n2')\n",
        "plt.plot(n3, torch.tensor(iter_grad_sigma).detach().numpy()[n3], label = 'grad n3')\n",
        "plt.plot(n4, torch.tensor(iter_grad_sigma).detach().numpy()[n4], label = 'grad n4')\n",
        "plt.plot(n5, torch.tensor(iter_grad_sigma).detach().numpy()[n5], label = 'grad n5')\n",
        "plt.ylabel('grad of sigma')\n",
        "plt.xlabel('iterations')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.plot(n1, torch.tensor(iter_grad_beta).detach().numpy()[n1], label = 'grad n1')\n",
        "plt.plot(n2, torch.tensor(iter_grad_beta).detach().numpy()[n2], label = 'grad n2')\n",
        "plt.plot(n3, torch.tensor(iter_grad_beta).detach().numpy()[n3], label = 'grad n3')\n",
        "plt.plot(n4, torch.tensor(iter_grad_beta).detach().numpy()[n4], label = 'grad n4')\n",
        "plt.plot(n5, torch.tensor(iter_grad_beta).detach().numpy()[n5], label = 'grad n5')\n",
        "plt.ylabel('grad of beta')\n",
        "plt.xlabel('iterations')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.plot(n1, torch.tensor(iter_grad_rho).detach().numpy()[n1], label = 'grad n1')\n",
        "plt.plot(n2, torch.tensor(iter_grad_rho).detach().numpy()[n2], label = 'grad n2')\n",
        "plt.plot(n3, torch.tensor(iter_grad_rho).detach().numpy()[n3], label = 'grad n3')\n",
        "plt.plot(n4, torch.tensor(iter_grad_rho).detach().numpy()[n4], label = 'grad n4')\n",
        "plt.plot(n5, torch.tensor(iter_grad_rho).detach().numpy()[n5], label = 'grad n5')\n",
        "plt.ylabel('grad of rho')\n",
        "plt.xlabel('iterations')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h8H5pVmCENoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(18, 4.5))\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.plot(range(len(iter_sigma_GD)), torch.tensor(iter_sigma_GD).detach().clone(), label = 'iter sigma')\n",
        "plt.plot(range(len(iter_sigma_GD)), true_sigma * np.ones((len(iter_sigma_GD),)), label = 'True sigma')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('values')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.plot(range(len(iter_beta_GD)), torch.tensor(iter_beta_GD).detach().clone(), label = 'iter beta')\n",
        "plt.plot(range(len(iter_beta_GD)), true_beta * np.ones((len(iter_beta_GD),)), label = 'True beta')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('values')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.plot(range(len(iter_rho_GD)), torch.tensor(iter_rho_GD).detach().clone(), label = 'iter_rho')\n",
        "plt.plot(range(len(iter_rho_GD)), true_rho * np.ones((len(iter_rho_GD),)), label = 'True rho')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('values')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MEdNqw35ENwd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}