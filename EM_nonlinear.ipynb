{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNolwV2/GabCDt8VkNTvp2g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BotaoJin/Code-for-Thesis/blob/main/EM_nonlinear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8snfZJjLAxT4",
        "outputId": "4560b489-d8ec-4344-8db3-f44e7412af49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchdiffeq\n",
            "  Downloading torchdiffeq-0.2.3-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq) (1.7.3)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from torchdiffeq) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy>=1.4.0->torchdiffeq) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->torchdiffeq) (4.1.1)\n",
            "Installing collected packages: torchdiffeq\n",
            "Successfully installed torchdiffeq-0.2.3\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "!pip install torchdiffeq\n",
        "from torchdiffeq import odeint\n",
        "from torchdiffeq import odeint_adjoint\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Lorenz63(nn.Module):\n",
        "  def __init__(self, sigma, beta, rho, x_dim = 3):\n",
        "      super().__init__()\n",
        "      self.sigma = nn.Parameter(sigma)\n",
        "      self.beta = nn.Parameter(beta)\n",
        "      self.rho = nn.Parameter(rho)\n",
        "      self.x_dim = 3\n",
        "\n",
        "  def forward(self, t, u):\n",
        "    sigma = self.sigma\n",
        "    beta = self.beta\n",
        "    rho = self.rho\n",
        "    out = torch.stack((sigma*(u[...,1]-u[...,0]), rho*u[..., 0]-u[..., 0]*u[..., 2]-u[..., 1], u[..., 0]*u[..., 1]-beta*u[..., 2]), dim = -1)\n",
        "    return out"
      ],
      "metadata": {
        "id": "ZF7NYsMgA2sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_sigma = torch.tensor(10.0)\n",
        "true_beta = torch.tensor(8.0/3.0)\n",
        "true_rho = torch.tensor(28.0)\n",
        "\n",
        "true_model = Lorenz63(sigma = true_sigma, beta = true_beta, rho = true_rho)"
      ],
      "metadata": {
        "id": "HFPhjWJCA2vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_dim = 3\n",
        "y_dim = 3\n",
        "H = torch.eye(3)\n",
        "cov = torch.eye(3)\n",
        "gamma = .1\n",
        "T_ = 100\n",
        "t1 = torch.tensor([0., .01])\n",
        "\n",
        "def generate_data(ode_func):\n",
        "  x0 = torch.tensor([1., 1., 1.])\n",
        "  X = torch.empty(T_+1, x_dim)\n",
        "  Y = torch.empty(T_, y_dim)\n",
        "  X[0] = x0\n",
        "\n",
        "  for t in range(T_):\n",
        "    X[t+1] = odeint(ode_func, X[t], t1)[-1]\n",
        "    X[t+1] = MultivariateNormal(X[t+1], gamma**2*cov).sample()\n",
        "    Y[t] = MultivariateNormal(X[t+1], gamma**2*cov).sample()\n",
        "\n",
        "  return X, Y"
      ],
      "metadata": {
        "id": "5FDZ2ucXA2yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate data\n",
        "num_data_set = 5\n",
        "X = torch.zeros(T_+1, num_data_set, x_dim)\n",
        "Y = torch.zeros(T_, num_data_set, y_dim)\n",
        "\n",
        "for i in range(num_data_set):\n",
        "  X_data, Y_data = generate_data(true_model)\n",
        "  X[:,i,:], Y[:,i,:] = X_data, Y_data\n",
        "\n",
        "X, Y = X.mean(-2), Y.mean(-2)"
      ],
      "metadata": {
        "id": "gxJqQ3c7A201"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EM Algorithm\n",
        "# E-step: sample V from MCMC Method\n",
        "def Sample_V_MCMC(ode_func, V, Y):\n",
        "  V_p = V + MultivariateNormal(torch.zeros(x_dim).expand(V.size()), 0.01**2*torch.eye(x_dim)).sample()\n",
        "\n",
        "  V_new = odeint(ode_func, V[:-1], t1)[-1]\n",
        "  V_p_new = odeint(ode_func, V_p[:-1], t1)[-1]\n",
        "  log_FV = -1/(2*gamma**2)*torch.linalg.norm(Y - V[1:])**2 - 1/(2*gamma**2)*torch.linalg.norm(V[1:] - V_new)**2\n",
        "  log_FV_p = -1/(2*gamma**2)*torch.linalg.norm(Y - V_p[1:])**2 - 1/(2*gamma**2)*torch.linalg.norm(V_p[1:] - V_p_new)**2\n",
        "\n",
        "  d1 = MultivariateNormal(V, 0.01*2*torch.eye(3))\n",
        "  d2 = MultivariateNormal(V_p, 0.01*2*torch.eye(3))\n",
        "\n",
        "  log_acc_prob1 = log_FV_p + torch.sum(d2.log_prob(V))\n",
        "  log_acc_prob2 = log_FV + torch.sum(d1.log_prob(V_p))\n",
        "  acc_prob = torch.exp(torch.min(torch.tensor([0., log_acc_prob1 - log_acc_prob2])))\n",
        "\n",
        "  a = torch.distributions.uniform.Uniform(torch.tensor([0.0]), torch.tensor([1.0])).sample()\n",
        "  if a <= acc_prob: # Accept\n",
        "    acc = 1\n",
        "    return acc, V_p.clone().detach().requires_grad_(True)\n",
        "  else: # Reject\n",
        "    acc = 0\n",
        "    return acc, V.clone().detach().requires_grad_(True)"
      ],
      "metadata": {
        "id": "un-jNeStBECQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EnKF(ode_func, Y, N_ensem, t_span, x0 = torch.tensor([1., 1., 1.])):\n",
        "  T = Y.shape[-2]\n",
        "  X = x0.expand((N_ensem, x_dim))\n",
        "  res = torch.empty(T+1, N_ensem, x_dim)\n",
        "  res[0] = X\n",
        "  mean = torch.zeros(x_dim)\n",
        "  log_likelihood = torch.tensor(0.)\n",
        "\n",
        "  for t in range(T):\n",
        "    # Forcast Step\n",
        "    X = odeint_adjoint(ode_func, X, t_span, method = 'rk4', adjoint_method = 'rk4')[-1]\n",
        "    X = X + MultivariateNormal(mean.expand((N_ensem, x_dim)), (gamma**2)*cov).sample() # model error for X: dim = (N_ensem, x_dim)\n",
        "    X_m = X.mean(dim = -2).unsqueeze(-2) # dim = (1, x_dim)\n",
        "    X_ct = X - X_m\n",
        "\n",
        "    # Analysis Step\n",
        "    y_obs_j = Y[t].unsqueeze(-2) # dim = (1, y_dim)\n",
        "    y_obs_perturb = MultivariateNormal(y_obs_j.expand(N_ensem, y_dim), (gamma**2)*cov).sample()\n",
        "\n",
        "    C_uu = 1/(N_ensem - 1)*X_ct.transpose(-1, -2)@X_ct # dim = (1, x_dim)\n",
        "    # In this model, setting H = I\n",
        "    HX = X\n",
        "    HX_m = X_m\n",
        "    HC = C_uu\n",
        "    HCH_T = HC\n",
        "    HCH_TR_chol = torch.linalg.cholesky(HCH_T + (gamma**2)*cov)\n",
        "    d = MultivariateNormal(HX_m.squeeze(-2), scale_tril = HCH_TR_chol)\n",
        "    log_likelihood += d.log_prob(y_obs_j.squeeze(-2))\n",
        "\n",
        "    # Update and store X_j^{1:N}\n",
        "    pre = (y_obs_perturb-HX)@torch.cholesky_inverse(HCH_TR_chol)\n",
        "    X = X + pre@HC\n",
        "    res[t+1] = X\n",
        "\n",
        "  return X, res, log_likelihood"
      ],
      "metadata": {
        "id": "uqyUwE8ZBEF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Exp_value(ode_func, res):\n",
        "  N = res.size(dim = -2)\n",
        "  T = res.size(dim = 0)\n",
        "  t1 = torch.tensor([0., .01])\n",
        "  exp_value = 0\n",
        "\n",
        "  for t in range(T-1):\n",
        "    V_hat = odeint_adjoint(ode_func, res[t], t1, method = 'rk4', adjoint_method = 'rk4')[-1]\n",
        "    V_diff = res[t+1] - V_hat\n",
        "    exp_value += torch.linalg.norm(V_diff, 'fro')**2\n",
        "\n",
        "  return 1/(N*gamma**2)*exp_value"
      ],
      "metadata": {
        "id": "C-iCMUZqBEI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EM algorithm with MCMC (sigma unknown)\n",
        "sigma = torch.tensor(0.01)\n",
        "beta = true_beta\n",
        "rho = true_rho\n",
        "J = 50\n",
        "I = 1\n",
        "eta = 0.05\n",
        "\n",
        "iter_sigma_EM_MCMC = [sigma]\n",
        "sample_size = 20\n",
        "sample_iter = 0\n",
        "num_acc = 0\n",
        "\n",
        "for j in range(J):\n",
        "  ode_func = Lorenz63(sigma = sigma, beta = beta, rho = rho)\n",
        "  V = torch.empty(T_+1, x_dim)\n",
        "  V[0] = torch.tensor([1., 1., 1.])\n",
        "  for t in range(T_):\n",
        "    V[t+1] = odeint(ode_func, V[t], t1)[-1] + MultivariateNormal(torch.zeros(x_dim), gamma**2*cov).sample()\n",
        "  res = torch.empty(T_+1, sample_size, x_dim)\n",
        "\n",
        "  for sz in range(sample_size):\n",
        "    acc, V = Sample_V_MCMC(ode_func, V, Y)\n",
        "    res[:,sz,:] = V\n",
        "    num_acc += acc\n",
        "    sample_iter += 1\n",
        "\n",
        "  for i in range(I):\n",
        "    #torch.autograd.set_detect_anomaly(True)\n",
        "    sigma1 = sigma.detach().requires_grad_(True)\n",
        "    ode_fun = Lorenz63(sigma1, beta, rho)\n",
        "    J = Exp_value(ode_fun, res)\n",
        "    J.backward(retain_graph = True)\n",
        "    grad_ = ode_fun.sigma.grad\n",
        "    sigma = sigma - eta * grad_\n",
        "    iter_sigma_EM_MCMC.append(sigma)"
      ],
      "metadata": {
        "id": "Sr4ZMi3nA23N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EM algorithm with MCMC (beta unknown)\n",
        "sigma = true_beta\n",
        "beta = torch.tensor(0.)\n",
        "rho = true_rho\n",
        "J = 50\n",
        "I = 1\n",
        "eta = 0.01\n",
        "\n",
        "iter_beta_EM_MCMC = [beta]\n",
        "sample_size = 20\n",
        "sample_iter = 0\n",
        "num_acc = 0\n",
        "\n",
        "for j in range(J):\n",
        "  ode_func = Lorenz63(sigma = sigma, beta = beta, rho = rho)\n",
        "  V = torch.empty(T_+1, x_dim)\n",
        "  V[0] = torch.tensor([1., 1., 1.])\n",
        "  for t in range(T_):\n",
        "    V[t+1] = odeint(ode_func, V[t], t1)[-1] + MultivariateNormal(torch.zeros(x_dim), gamma**2*cov).sample()\n",
        "  res = torch.empty(T_+1, sample_size, x_dim)\n",
        "\n",
        "  for sz in range(sample_size):\n",
        "    acc, V = Sample_V_MCMC(ode_func, V, Y)\n",
        "    res[:,sz,:] = V\n",
        "    num_acc += acc\n",
        "    sample_iter += 1\n",
        "\n",
        "  for i in range(I):\n",
        "    #torch.autograd.set_detect_anomaly(True)\n",
        "    beta1 = beta.detach().requires_grad_(True)\n",
        "    ode_fun = Lorenz63(sigma, beta1, rho)\n",
        "    J = Exp_value(ode_fun, res)\n",
        "    J.backward(retain_graph = True)\n",
        "    grad_ = ode_fun.beta.grad\n",
        "    beta = beta - eta * grad_\n",
        "    iter_beta_EM_MCMC.append(beta)"
      ],
      "metadata": {
        "id": "1UWonNusBk0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EM algorithm with MCMC (rho unknown)\n",
        "sigma = true_sigma\n",
        "beta = true_beta\n",
        "rho = torch.tensor(0.01)\n",
        "J = 50\n",
        "I = 1\n",
        "eta = 0.5\n",
        "\n",
        "iter_rho_EM_MCMC = [rho]\n",
        "sample_size = 20\n",
        "sample_iter = 0\n",
        "num_acc = 0\n",
        "\n",
        "for j in range(J):\n",
        "  ode_func = Lorenz63(sigma = sigma, beta = beta, rho = rho)\n",
        "  V = torch.empty(T_+1, x_dim)\n",
        "  V[0] = torch.tensor([1., 1., 1.])\n",
        "  for t in range(T_):\n",
        "    V[t+1] = odeint(ode_func, V[t], t1)[-1] + MultivariateNormal(torch.zeros(x_dim), gamma**2*cov).sample()\n",
        "  res = torch.empty(T_+1, sample_size, x_dim)\n",
        "\n",
        "  for sz in range(sample_size):\n",
        "    acc, V = Sample_V_MCMC(ode_func, V, Y)\n",
        "    res[:,sz,:] = V\n",
        "    num_acc += acc\n",
        "    sample_iter += 1\n",
        "\n",
        "  for i in range(I):\n",
        "    #torch.autograd.set_detect_anomaly(True)\n",
        "    rho1 = rho.detach().requires_grad_(True)\n",
        "    ode_fun = Lorenz63(sigma, beta, rho1)\n",
        "    J = Exp_value(ode_fun, res)\n",
        "    J.backward(retain_graph = True)\n",
        "    grad_ = ode_fun.rho.grad\n",
        "    rho = rho - eta * grad_\n",
        "    iter_rho_EM_MCMC.append(rho)"
      ],
      "metadata": {
        "id": "gTiQeFQBBrzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EM algo for EnKF\n",
        "sigma, beta, rho = torch.tensor(.01), torch.tensor(0.), torch.tensor(0.)\n",
        "#sigma, beta, rho = torch.tensor(.01), true_beta, true_rho\n",
        "#sigma, beta, rho = true_sigma, torch.tensor(0.), true_rho\n",
        "#sigma, beta, rho = true_sigma, true_beta, torch.tensor(0.)\n",
        "ode_func = Lorenz63(sigma = sigma, beta = beta, rho = rho)\n",
        "\n",
        "J = 50\n",
        "I = 1\n",
        "eta = 1e-3\n",
        "\n",
        "iter_sigma_EM_EnKF = [sigma]\n",
        "iter_beta_EM_EnKF = [beta]\n",
        "iter_rho_EM_EnKF = [rho]\n",
        "\n",
        "for j in range(J):\n",
        "  # E-step: sample V from P(V|Y)\n",
        "  x, res, loglike = EnKF(ode_func, Y, N_ensem = 20, t_span = t1)\n",
        "\n",
        "  # M-step: Maximize the Expected Value using Gradient Descent\n",
        "  for i in range(I):\n",
        "    sigma1 = sigma.clone().detach().requires_grad_(True)\n",
        "    beta1 = beta.clone().detach().requires_grad_(True)\n",
        "    rho1 = rho.clone().detach().requires_grad_(True)\n",
        "\n",
        "    ode_func = Lorenz63(sigma = sigma1, beta = beta1, rho = rho1)\n",
        "    J = Exp_value(ode_func, res)\n",
        "    J.backward(retain_graph = True)\n",
        "\n",
        "    grad_sigma = ode_func.sigma.grad\n",
        "    grad_beta = ode_func.beta.grad\n",
        "    grad_rho = ode_func.rho.grad\n",
        "\n",
        "    sigma = sigma - 2*eta*grad_sigma\n",
        "    beta = beta - 0.3*eta*grad_beta\n",
        "    rho = rho - eta*grad_rho\n",
        "\n",
        "    iter_sigma_EM_EnKF.append(sigma)\n",
        "    iter_beta_EM_EnKF.append(beta)\n",
        "    iter_rho_EM_EnKF.append(rho)"
      ],
      "metadata": {
        "id": "RsKVpJYJBxEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fig = plt.figure(figsize=(18, 4.5))\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.plot(range(len(iter_sigma_EM_MCMC)), torch.tensor(iter_sigma_EM_MCMC).detach().clone(), label = 'MCMC')\n",
        "plt.plot(range(len(iter_sigma_EM_EnKF)), torch.tensor(iter_sigma_EM_EnKF).detach().clone(), label = 'EnKF')\n",
        "plt.plot(range(len(iter_sigma_EM_MCMC)), true_sigma * np.ones((len(iter_sigma_EM_MCMC),)), label = 'True sigma')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('values')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.plot(range(len(iter_beta_EM_MCMC)), torch.tensor(iter_beta_EM_MCMC).detach().clone(), label = 'MCMC')\n",
        "plt.plot(range(len(iter_beta_EM_EnKF)), torch.tensor(iter_beta_EM_EnKF).detach().clone(), label = 'EnKF')\n",
        "plt.plot(range(len(iter_beta_EM_MCMC)), true_beta * np.ones((len(iter_beta_EM_MCMC),)), label = 'True beta')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('values')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.plot(range(len(iter_rho_EM_MCMC)), torch.tensor(iter_rho_EM_MCMC).detach().clone(), label = 'MCMC')\n",
        "plt.plot(range(len(iter_rho_EM_EnKF)), torch.tensor(iter_rho_EM_EnKF).detach().clone(), label = 'EnKF')\n",
        "plt.plot(range(len(iter_rho_EM_MCMC)), true_rho * np.ones((len(iter_rho_EM_EnKF),)), label = 'True rho')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('values')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "okS-KQxUB87q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}