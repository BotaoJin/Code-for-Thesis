{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP5Gwg2NxOXxml8FUiOY7Fa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BotaoJin/Code-for-Thesis/blob/main/EM_linear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_6jnQZd1Pto"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T_ = 100\n",
        "sigma = 0.1\n",
        "gamma = 0.1\n",
        "x_dim = 2\n",
        "y_dim = 2\n",
        "mean = torch.zeros(x_dim)\n",
        "cov = torch.eye(x_dim)\n",
        "eps = 0.01"
      ],
      "metadata": {
        "id": "ONEA0i_X1hK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def KFilter(theta):\n",
        "    x0 = torch.tensor([1., 2.])# initialization of X0\n",
        "    X = torch.zeros(T_, x_dim)\n",
        "    Y = torch.zeros(T_, y_dim)\n",
        "    A_theta = torch.diag(torch.tensor(theta))\n",
        "\n",
        "    for t in range(T_):\n",
        "        zeta = MultivariateNormal(mean, (sigma**2)*cov)\n",
        "        eta = MultivariateNormal(mean, (gamma**2)*cov)\n",
        "        if t == 0:\n",
        "            X[t,:] = x0@A_theta + zeta.sample()\n",
        "        else:\n",
        "            x = X[t-1,:]\n",
        "            X[t,:] = x@A_theta + zeta.sample()\n",
        "            \n",
        "        Y[t,:] = X[t,:] + eta.sample()\n",
        "        \n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "TxhXJKNt1hRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generating data\n",
        "num_data_set = 1\n",
        "X = torch.zeros(T_, num_data_set, x_dim)\n",
        "Y = torch.zeros(T_, num_data_set, y_dim)\n",
        "for i in range(num_data_set):\n",
        "  X_data, Y_data = KFilter([.9, .8])\n",
        "  X[:, i, :], Y[:, i, :] = X_data, Y_data\n",
        "\n",
        "X, Y = X.mean(dim = -2), Y. mean(dim = -2)"
      ],
      "metadata": {
        "id": "K6XdufWf1hTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EM Algorithm\n",
        "# E-step: sample V from MCMC Method\n",
        "def Sample_V_MCMC(theta, V, Y):\n",
        "  V_p = V + MultivariateNormal(torch.zeros(x_dim).expand(V.size()), 0.01**2*torch.eye(x_dim)).sample()\n",
        "\n",
        "  log_FV = -1/(2*gamma**2)*torch.linalg.norm(Y - V[1:])**2 - 1/(2*sigma**2)*torch.linalg.norm(V[1:] - V[:-1]*theta)**2\n",
        "  log_FV_p = -1/(2*gamma**2)*torch.linalg.norm(Y - V_p[1:])**2 - 1/(2*sigma**2)*torch.linalg.norm(V_p[1:] - V_p[:-1]*theta)**2\n",
        "\n",
        "  d1 = MultivariateNormal(V, 0.01*2*torch.eye(2))\n",
        "  d2 = MultivariateNormal(V_p, 0.01*2*torch.eye(2))\n",
        "\n",
        "  log_acc_prob1 = log_FV_p + torch.sum(d2.log_prob(V))\n",
        "  log_acc_prob2 = log_FV + torch.sum(d1.log_prob(V_p))\n",
        "  acc_prob = torch.exp(torch.min(torch.tensor([0., log_acc_prob1 - log_acc_prob2])))\n",
        "\n",
        "  a = torch.distributions.uniform.Uniform(torch.tensor([0.0]), torch.tensor([1.0])).sample()\n",
        "  if a <= acc_prob: # Accept\n",
        "    acc = 1\n",
        "    return acc, V_p\n",
        "  else: # Reject\n",
        "    acc = 0\n",
        "    return acc, V"
      ],
      "metadata": {
        "id": "teqqG-qi1hVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EM Algorithm\n",
        "# E-step: sample V from P(V|Y, theta) with EnKF Method\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "\n",
        "def Sample_V_EnKF(theta, Y, N_ensem, x0 = torch.tensor([1., 2.])):\n",
        "  T = Y.shape[-2]\n",
        "  X = x0.expand((N_ensem, x_dim))\n",
        "  res = torch.empty(T+1, N_ensem, x_dim)\n",
        "  res[0] = X\n",
        "\n",
        "  for j in range(T):\n",
        "    # Forcast Step\n",
        "    X = X * theta\n",
        "    #X = X @ A_theta\n",
        "    X = X + MultivariateNormal(mean.expand(N_ensem, x_dim), (sigma**2)*cov).sample() # model error for X: dim = (N_ensem, x_dim)\n",
        "    X_m = X.mean(dim = -2).unsqueeze(-2) # dim = (1, x_dim)\n",
        "    X_ct = X - X_m\n",
        "        \n",
        "    # Analysis Step: for $A_{\\theta}$ is a linear operator\n",
        "    y_obs_j = Y[j].unsqueeze(-2) # dim = (1, y_dim)\n",
        "    y_obs_perturb = MultivariateNormal(y_obs_j.expand(N_ensem, y_dim), (gamma**2)*cov).sample()\n",
        "        \n",
        "    C_uu = 1/(N_ensem - 1)*X_ct.transpose(-1, -2)@X_ct # dim = (1, x_dim)\n",
        "    # In this model, setting H = I\n",
        "    HX = X\n",
        "    HX_m = X_m\n",
        "    HC = C_uu\n",
        "    HCH_T = HC\n",
        "    HCH_TR_chol = torch.linalg.cholesky(HCH_T + (gamma**2)*cov)\n",
        "    \n",
        "    # Update and store X_j^{1:N}\n",
        "    pre = (y_obs_perturb-HX)@torch.cholesky_inverse(HCH_TR_chol)\n",
        "    X = X + pre@HC\n",
        "    res[j+1] = X\n",
        "    \n",
        "  return res"
      ],
      "metadata": {
        "id": "T8V9W9NZ1hX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EM Algorithm\n",
        "# M-step for calculating expected value\n",
        "def Exp_value(theta, res):\n",
        "  T = res.size(dim = 0)\n",
        "  N = res.size(dim = -2)\n",
        "  exp_val = 0\n",
        "\n",
        "  for t in range(T-1):\n",
        "    Y_p = res[t+1] - res[t]*theta\n",
        "    exp_val += torch.linalg.norm(Y_p, 'fro')**2\n",
        "\n",
        "  return 1./(N * gamma**2)*exp_val"
      ],
      "metadata": {
        "id": "r9pVChkk1hZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EM algorithm with MCMC\n",
        "L = 600\n",
        "I = 1\n",
        "theta = torch.tensor([0., 0.])\n",
        "eta = 3e-4\n",
        "iter_theta1_EM_MCMC = [theta[0]]\n",
        "iter_theta2_EM_MCMC = [theta[1]]\n",
        "sample_size = 100\n",
        "sample_iter = 0\n",
        "num_acc = 0\n",
        "\n",
        "for l in range(L):\n",
        "  V = torch.empty(T_+1, x_dim)\n",
        "  V[0] = torch.tensor([1., 2.])\n",
        "  res = torch.empty(T_+1, sample_size, x_dim)\n",
        "  for t in range(T_):\n",
        "    V[t+1] = V[t]*theta + MultivariateNormal(torch.zeros(x_dim), sigma**2*cov).sample()\n",
        "\n",
        "  for sz in range(sample_size):\n",
        "    acc, V = Sample_V_MCMC(theta, V, Y)\n",
        "    res[:,sz,:] = V\n",
        "    sample_iter += 1\n",
        "    num_acc += acc\n",
        "\n",
        "  for i in range(I):\n",
        "    theta1 = theta.clone().detach().requires_grad_(True)\n",
        "    J = Exp_value(theta1, res)\n",
        "    J.backward()\n",
        "    grad_ = theta1.grad\n",
        "    theta = theta - eta*torch.tensor([3., 1.])*grad_\n",
        "    #print(J, grad_)\n",
        "    iter_theta1_EM_MCMC.append(theta[0])\n",
        "    iter_theta2_EM_MCMC.append(theta[1])"
      ],
      "metadata": {
        "id": "F_ATxLjw169X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EM algorithm with EnKF Method\n",
        "L = 600\n",
        "N_ensem = 100\n",
        "I = 1\n",
        "theta = torch.tensor([0., 0.])\n",
        "eta = 1e-4\n",
        "alpha = 0.5\n",
        "iter_theta1_EM_EnKF = [theta[0]]\n",
        "iter_theta2_EM_EnKF = [theta[1]]\n",
        "\n",
        "for l in range(L):\n",
        "  # E_step: Sample V\n",
        "  res = Sample_V_EnKF(theta, Y, N_ensem)\n",
        "    \n",
        "  # M_step:\n",
        "  for i in range(I):\n",
        "    theta1 = theta.clone().detach().requires_grad_(True)\n",
        "    J = Exp_value(theta1, res)\n",
        "    J.backward()\n",
        "    grad_ = theta1.grad\n",
        "    #print(grad_)\n",
        "    theta = theta - eta*grad_\n",
        "    \n",
        "  iter_theta1_EM_EnKF.append(theta[0])\n",
        "  iter_theta2_EM_EnKF.append(theta[1])"
      ],
      "metadata": {
        "id": "9t6DoEjp2grW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(12, 4.5))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(range(len(iter_theta1_EM_MCMC)), torch.tensor(iter_theta1_EM_MCMC).detach().numpy(), label = 'MCMC')\n",
        "plt.plot(range(len(iter_theta1_EM_EnKF)), torch.tensor(iter_theta1_EM_EnKF).detach().numpy(), label = 'EnKF')\n",
        "plt.plot(range(len(iter_theta1_EM_MCMC)), .9*np.ones((len(iter_theta1_EM_MCMC),)), label = 'True value')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('values of theta1')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(range(len(iter_theta2_EM_MCMC)), torch.tensor(iter_theta2_EM_MCMC).detach().numpy(), label = 'MCMC')\n",
        "plt.plot(range(len(iter_theta2_EM_EnKF)), torch.tensor(iter_theta2_EM_EnKF).detach().numpy(), label = 'EnKF')\n",
        "plt.plot(range(len(iter_theta2_EM_MCMC)), .8*np.ones((len(iter_theta2_EM_MCMC),)), label = 'True value')\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('values of theta2')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ven3yz-g2vps"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}